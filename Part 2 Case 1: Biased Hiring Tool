
✅ Case 1: Biased Hiring Tool – Amazon
Scenario Summary
Amazon’s AI-powered recruiting tool unintentionally discriminated against female candidates. The tool was trained on 10 years of historical hiring data—primarily male applicants in tech roles—which led the AI to favor male-associated patterns and penalize resumes with female indicators.

1. Source of Bias
Training Data Bias: The model learned from male-dominated resumes, reinforcing past hiring discrimination.

Feature Bias: It downgraded resumes that included gendered terms like “women’s” or referenced all-women colleges.

Lack of Fairness Controls: The model lacked mechanisms to detect and correct bias during development.

2. Three Fixes to Make the Tool Fairer
Diversify the Training Data

Retrain using gender-balanced resumes that represent a wider range of qualified candidates.

Remove Gender Proxies

Strip out or mask features that serve as indirect gender indicators, like names or club affiliations.

Use Fairness-Aware Algorithms

Apply methods like reweighing, adversarial debiasing, or fair post-processing to ensure equitable treatment.

3. Fairness Evaluation Metrics
Disparate Impact Ratio: Should fall between 0.8 and 1.25 for gender fairness.

Equal Opportunity Difference: Compare true positive rates between male and female applicants.

False Positive/Negative Rate Comparison: Monitor misclassification rates by gender.

Statistical Parity Difference: Measure selection rate gap between groups (target ≈ 0).
